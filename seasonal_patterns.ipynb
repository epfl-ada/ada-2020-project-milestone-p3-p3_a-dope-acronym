{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeohash as pgh\n",
    "import matplotlib.pyplot as plt\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=True, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Seasonal Patterns in User Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more to human movement than periodic patterns and social-based movement. While the paper accurately predicts human movement throughout the week, we propose a more all-encompassing analysis that paints a broader picture, exploring monthly and seasonal movement. To do so, we will use the original check-in and friendship datasets from Gowalla and Brightkite and separate them into subsets representing a narrower time of the year. We want to see what mobility patterns are exhibited in different countries and throughout different times of the year: users can go on vacation, travel abroad, and change homes. This would allow us to understand the travelling patterns of users around the world (i.e. who is more likely to travel and where) as well as the distribution of check-ins and their density. We want to produce visualizations showcasing these patterns on a world map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starting point is the first replication of the Friendship and Mobility paper. Below, the data is imported and cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets have no headers, so we have to name them for clarity\n",
    "checkin_header = ['user', 'checkin_time', 'latitude', 'longitude', 'location_id']\n",
    "edges_header = ['user1', 'user2']\n",
    "\n",
    "# Load the data by specifying the correct compression algorithm, separator and column names\n",
    "checkin_brightkite_orig = pd.read_csv('data/loc-brightkite_totalCheckins.txt.gz', compression = 'gzip', sep = '\\t', names = checkin_header)\n",
    "edges_brightkite_orig = pd.read_csv('data/loc-brightkite_edges.txt.gz', compression = 'gzip', sep = '\\t', names = edges_header)\n",
    "checkin_gowalla_orig = pd.read_csv('data/loc-gowalla_totalCheckins.txt.gz', compression = 'gzip', sep = '\\t', names = checkin_header)\n",
    "edges_gowalla_orig = pd.read_csv('data/loc-gowalla_edges.txt.gz', compression = 'gzip', sep = '\\t', names = edges_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the latitude and longitude values are not in the valid range of [-90, 90] and [-180, 180] respectively. Moreover, the location having (lat, long) = (0,0) is not a valid one . In both datasets, we remove rows in which either of the location attributes are not valid. We also remove the rows of the Brightkite dataset that have NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(checkins):\n",
    "    \"\"\" Rids the check-in data of invalid coordinates \"\"\"\n",
    "    to_drop = []\n",
    "    # Select indexes to drop depending on conditions mentionned above \n",
    "    to_drop.append(checkins[(checkins['latitude'] < -90.0) | \n",
    "                                   (checkins['latitude'] > 90.0)].index)\n",
    "    to_drop.append(checkins[(checkins['longitude'] < -180.0) | \n",
    "                                   (checkins['longitude'] > 180.0)].index)\n",
    "    to_drop.append(checkins[(checkins['latitude'] == 0) & \n",
    "                                   (checkins['longitude'] == 0)].index)\n",
    "    \n",
    "    for item in to_drop:\n",
    "        checkins.drop(item, inplace = True)\n",
    "    \n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass copy of DataFrama in order not to have to reload the original one in case of a mistake\n",
    "checkin_gowalla    = clean(checkin_gowalla_orig.copy())\n",
    "checkin_brightkite = clean(checkin_brightkite_orig.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding country information to check-ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each check-in, we append the country code corresponding to the country the check-in was made in. This is done using a reverse geohashing library [`reverse_geocoder`](https://github.com/thampiman/reverse-geocoder) that maps coordinates (latitutde, longitude) to information about a specific place (e.g. city, country, country code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(row):\n",
    "    return (row['latitude'], row['longitude'])\n",
    "\n",
    "def coordinates_to_country(df):\n",
    "    \"\"\"\n",
    "        Maps the coordinates of a dataframe to the country\n",
    "        code using reverse geocoding\n",
    "    \"\"\"\n",
    "    coordinates_tuples = list(df.parallel_apply(extract_coordinates, axis = 1))\n",
    "    geocodes = rg.search(coordinates_tuples)\n",
    "    return [gc['cc'] for gc in geocodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append country codes to each check-in for both datasets\n",
    "checkin_brightkite['cc'] = coordinates_to_country(checkin_brightkite)\n",
    "checkin_gowalla['cc']    = coordinates_to_country(checkin_gowalla)\n",
    "\n",
    "checkin_brightkite.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the location of user homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the geohash is used to determine the home location for every user. Since this is similar to what was done in the replication, we will not go into detail about the implementation and justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geohash_encode(row):\n",
    "    \"\"\"\n",
    "        Computes and returns the geohash with \n",
    "        precision 4 for a given row \n",
    "    \"\"\"\n",
    "    return pgh.encode(*extract_coordinates(row), precision=4)\n",
    "\n",
    "def find_home(checkins, user_id):\n",
    "    \"\"\"\n",
    "        Given a user id, finds the most common world cell and \n",
    "        computes average location to estimate home location\n",
    "    \"\"\"\n",
    "    # Retrieve all the user's check-ins\n",
    "    group = checkins[checkins['user']  == user_id]\n",
    "    # Get the most common geohash for the user\n",
    "    geohash = group.groupby('geohash').size().idxmax()\n",
    "    \n",
    "    # Keep all checkins that happened in the most common geohash and compute average position\n",
    "    return group[group['geohash'] == geohash][['latitude', 'longitude']].mean()\n",
    "\n",
    "def find_home_brightkite(user_id):\n",
    "    return find_home(checkin_brightkite, user_id)\n",
    "\n",
    "def find_home_gowalla(user_id):\n",
    "    return find_home(checkin_gowalla, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the geohash for each check-in entry: that will determine the cell in which each check-in belongs\n",
    "checkin_brightkite['geohash'] = checkin_brightkite.parallel_apply(geohash_encode, axis = 1)\n",
    "checkin_gowalla['geohash']    = checkin_gowalla.parallel_apply(geohash_encode, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the users that made at least 1 check-in\n",
    "brightkite_users = set(checkin_brightkite.user)\n",
    "gowalla_users = set(checkin_gowalla.user)\n",
    "\n",
    "# Create empty DataFrame with user ids and apply function to all users\n",
    "brightkite_homes = pd.DataFrame(index = brightkite_users).index.to_series() \\\n",
    "    .parallel_apply(find_home_brightkite) \n",
    "\n",
    "gowalla_homes = pd.DataFrame(index = gowalla_users).index.to_series() \\\n",
    "    .parallel_apply(find_home_gowalla) \n",
    "\n",
    "# Add the country codes to the homes\n",
    "brightkite_homes['cc'] = coordinates_to_country(brightkite_homes)\n",
    "gowalla_homes['cc'] = coordinates_to_country(gowalla_homes)\n",
    "\n",
    "brightkite_homes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding temporal information to the check-in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to separate check-ins temporally, we add to each check-in, the month and season it was made in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(row):\n",
    "    return datetime.strptime(row['checkin_time'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def get_month(row):\n",
    "    \"\"\" Returns the string found in the row to the correctly formatted datetime object \"\"\"\n",
    "    return to_datetime(row).month\n",
    "\n",
    "# Define the datetime intervals according to every season\n",
    "Y = 2000 # Leap year to allow input X-02-29, which allows for leap days\n",
    "seasons = [('winter', (datetime(Y,  1,  1),  datetime(Y,  3, 20))),\n",
    "           ('spring', (datetime(Y,  3, 21),  datetime(Y,  6, 20))),\n",
    "           ('summer', (datetime(Y,  6, 21),  datetime(Y,  9, 22))),\n",
    "           ('autumn', (datetime(Y,  9, 23),  datetime(Y, 12, 20))),\n",
    "           ('winter', (datetime(Y, 12, 21),  datetime(Y, 12, 31)))]\n",
    "\n",
    "def get_season(row):\n",
    "    \"\"\" \n",
    "        Returns the season from a certain row\n",
    "        Adapted from https://stackoverflow.com/a/28688724 \n",
    "    \"\"\"\n",
    "    date_time = to_datetime(row)\n",
    "    date_time = date_time.replace(year=Y, hour=0, minute=0, second=0)\n",
    "    \n",
    "    return next(season for season, (start, end) in seasons if start <= date_time <= end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the month associated with every check-in\n",
    "checkin_brightkite['month'] = checkin_brightkite.parallel_apply(get_month, axis=1)\n",
    "checkin_gowalla['month'] = checkin_gowalla.parallel_apply(get_month, axis=1)\n",
    "\n",
    "# Find the season associated with every check-in\n",
    "checkin_brightkite['season'] = checkin_brightkite.parallel_apply(get_season, axis=1)\n",
    "checkin_gowalla['season'] = checkin_gowalla.parallel_apply(get_season, axis=1)\n",
    "\n",
    "checkin_brightkite.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify travelers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now indentify users who travel. We define a traveler as a user who has made at least one check-in outside of his home country (determined by the country of its home location). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traveler_check(group):\n",
    "    \"\"\" A user is a traveler if he has ever checked in to more than one country \"\"\"\n",
    "    return True if len(set(group['cc'])) > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find travellers in both datasets\n",
    "brightkite_homes['traveler'] = checkin_brightkite.groupby('user').parallel_apply(traveler_check)\n",
    "gowalla_homes['traveler'] = checkin_gowalla.groupby('user').parallel_apply(traveler_check)\n",
    "\n",
    "brightkite_homes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(checkin_brightkite, open('pickles/checkin_brightkite', 'wb'))\n",
    "pickle.dump(checkin_gowalla, open('pickles/checkin_gowalla', 'wb'))\n",
    "\n",
    "pickle.dump(brightkite_homes, open('pickles/brightkite_homes', 'wb'))\n",
    "pickle.dump(gowalla_homes, open('pickles/gowalla_homes', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
